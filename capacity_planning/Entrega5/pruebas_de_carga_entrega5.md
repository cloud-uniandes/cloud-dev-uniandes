# Pruebas de Capacidad - Entrega 5
---

## Objetivo General

Evaluar la **capacidad m√°xima y estabilidad** del sistema bajo condiciones de carga progresivas con la nueva arquitectura de **mensajer√≠a as√≠ncrona (SQS)** y **auto scaling**, identificando:

- El **punto de saturaci√≥n** de la capa web (usuarios concurrentes)
- La **degradaci√≥n del tiempo de respuesta** bajo cargas crecientes
- El **comportamiento del auto scaling** bajo diferentes cargas

---

## Infraestructura Evaluada

| Componente | Especificaci√≥n | Funci√≥n |
|------------|----------------|---------|
| **Load Balancer** | AWS ELB Application | Distribuci√≥n de tr√°fico HTTP |
| **Web API (Auto Scaled)** | ECS Fargate<br>Min: 1, Max: 3 | FastAPI + Nginx |
| **Worker** | ECS Fargate<br>Min: 1, Max: 3 | Celery + FFmpeg |
| **Message Queue** | Amazon SQS FIFO | Cola de tareas as√≠ncronas |
| **Base de Datos** | RDS PostgreSQL db.t3.micro Multi-AZ | Metadatos y estado |
| **Storage** | Amazon S3 | Almacenamiento de videos |
| **Monitoreo** | AWS CloudWatch + Auto Scaling Policies | M√©tricas y alarmas |

---

## Cambios Arquitect√≥nicos vs Entrega 4

| Aspecto | Entrega 4 | Entrega 5 |
|---------|-----------|-----------|
| **Capa Worker** | Instancias EC2 | Uso de ECS Fargate |
| **Capa API** | Instancias EC2 | Uso de ECS Fargate |
---

# Plan A: Capacidad de la Capa Web 

---

## Objetivo del Escenario

Determinar el n√∫mero m√°ximo de **usuarios concurrentes** que la API de subida soporta cumpliendo los SLOs:
- **Tiempo promedio de respuesta** ‚â§ 10,000 ms
- **Tasa de error (4xx/5xx)** ‚â§ 5%
- **Sin degradaci√≥n progresiva** en pruebas sostenidas

---

## Configuraci√≥n de la Prueba

Para el desarrollo de estas pruebas se contemplaron 3 escenarios. El primer escenario consisti√≥ en una prueba de sanidad con la que se pretend√≠a revisar el comportamiento b√°sico del sistema. El segundo escenario consiste en una prueba de escalamiento r√°pido en donde se genera un incremento en la cantidad de usuarios concurrentes. Finalmente, se realiz√≥ una prueba sostenida en donde se mantuvo la ejecuci√≥n por 5 minutos con el 80% de la carga encontrada como m√°xima en el anterior escenario. Estas pruebas fueron realizadas con Locust usando el archivo `capaWeb/plan_a_web_capacity.py`. Este archivo es una variaci√≥n del utilizado en la entrega 4 en donde se redefine la URL de la aplicaci√≥n y se agregan los endpoints faltantes para una prueba m√°s completa.  
### Escenario 1: prueba de sanidad 
| Par√°metro | Valor |
|-----------|-------|
| **Herramienta** | Locust 2.15+ |
| **Tipo de prueba** | Smoke Test |
| **Estrategia de carga** | 5 usuarios permamentes |
| **Duraci√≥n por fase** | 1 minuto|
---
### Escenario 2: prueba de escalamiento r√°pido
| Par√°metro | Valor |
|-----------|-------|
| **Herramienta** | Locust 2.15+ |
| **Tipo de prueba** | Ramp test |
| **Estrategia de carga** | 100 usuarios a 600 |
| **Duraci√≥n de fase** | 3 minutos de ramp up y 5 minutos sostenidos|
---
### Escenario 3: prueba sostenida
| Par√°metro | Valor |
|-----------|-------|
| **Herramienta** | Locust 2.15+ |
| **Tipo de prueba** | Stability test |
| **Estrategia de carga** | 80% de m√°xima carga de usuarios |
| **Duraci√≥n de fase** | 3 minutos de ramp up y 5 minutos sostenidos|
---
## Resultados Escenario 1
A continuaci√≥n, se muestran los resultados del escenario 1. En este se realiz√≥ la revisi√≥n de sanidad del sistema. En la siguiente imagen se muestra la tabla de resumen obtenida con locust en donde se puede ver que no se tiene ning√∫n error en las peticiones realizadas y que el tiempo de respuesta promedio es de 983 ms. Con esto, la prueba cumple con su objetivo de permitir la primera revisi√≥n de la capa web. 
![resumen sanity](capaWeb/sanity/sanity_smoke.png)

Adicionalmente, se incluyen las gr√°ficas de Locust con las que se puede ver el incremento a 5 usuarios y el flujo de peticiones por segundo y tiempos de respuesta. Con estas visualizaciones se puede ver que hay un comportamiento exitoso de la aplicaci√≥n en una primera instancia. 

![charts sanity](capaWeb/sanity/sanity_charts.png)
---
## Resultados Escenario 2
Dentro del segundo escenario se realiz√≥ una evaluaci√≥n del sistema en cuanto a la cantidad de usuarios concurrentes que soporta antes de presentarse degradaci√≥n en el sistema. Este proceso se realiz√≥ primeramente sobre el servicio sin opci√≥n de autoescalado y luego con opci√≥n de escalado hasta 3 instancias. Esto permite ver el desempe√±o general del sistema y comprobar el uso de autoescalamiento para aumentar el nivel de carga que puede manejar. 
### Casos sin escalado
Dentro del caso sin escalado se prob√≥ el desempe√±o con 100 y 200 usuarios concurrentes. A continuaci√≥n, se presentan las gr√°ficas de peiticiones por segundo, tiempo de respuesta y n√∫mero de usuarios. 
Por el lado de la gr√°fica de 100 usuarios, se puede ver que hay una degradaci√≥n del sistema cuando se tienen los 100 usuarios de forma concurrente. Esto hace que el n√∫mero de fallos y el tiempo por petici√≥n incremente a medida que se mantienen los usuarios. 
![charts ramp 100](capaWeb/Ramp/chart100uses.png)
Asimismo, se puede ver que en el momento en el que se tienen los 200 usuarios de forma concurrente se degrada considerablemente el sistema y su desempe√±o. 
![charts ramp 200](capaWeb/Ramp/200_ns_graph.png)
Para comparar m√°s f√°cilmente, se presenta la tabla con las m√©tricas principales obtenidas de estas pruebas. 
| **Cantidad de usuarios** | **Cantidad de peticiones** | **Cantidad Fallos** | **Porcentaje de fallo** | **Tiempo promedio (ms)** | **Tiempo m√°ximo (ms)** | **Req/s**  | **Failures/s** |
|--------------------------|----------------------------|---------------------|-------------------------|--------------------------|------------------------|------------|----------------|
| 100                      | 1709                       | 105                 | 6%                      | 6101.07                  | 42850.797              | 9.4954     | 0.5834         |
| 200                      | 4192                       | 291                 | 7%                      | 6858.22                  | 104631.838             | 13.97      | 0.9702         |

Como se puede ver al tener una √∫nica instancia se genera una degradaci√≥n mucho m√°s r√°pido comparado con los resultados encontrados dentro de las pruebas de la entrega pasada. 
### **Casos con escalado**
Los resultados sin escalado soportan una baja carga de usuarios, por lo que se prob√≥ la aplicaci√≥n al activar autoescalamiento hasta 3 tasks. Con esto se revis√≥ el comportamiento del sistema nuevamente. A continuaci√≥n, se presenta una tabla con los resultados para diferentes niveles de usuarios. 
| **Cantidad de usuarios** | **Cantidad de peticiones** | **Cantidad Fallos** | **Porcentaje de fallo** | **Tiempo promedio (ms)** | **Tiempo m√°ximo (ms)** | **Req/s**  | **Failures/s** |
|--------------------------|----------------------------|---------------------|-------------------------|--------------------------|------------------------|------------|----------------|
| 200                      | 5357                       | 12                  | 0.22%                   | 5507.32                  | 102207.2715            | 17.86      | 0.04           |
| 400                      | 8705                       | 11                  | 0.13%                   | 6828.117                 | 81645.514              | 28.9814    | 0.037          |
| 500                      | 6683                       | 170                 | 2.54%                   | 9549.010                 | 102846.3386            | 22.26      | 0.56           |
| 600                      | 8934                       | 480                 | 5.37%                   | 10195.8259               | 62443.8                | 29.723     | 1.597          |

Como se puede ver el punto de degradaci√≥n en donde la aplicaci√≥n deja de cumplir con los requisitos propuestso de contar con un error de peticiones menor al 5% y un tiempo de respuesta menor a los 10 segundos. Al comparar esto con los resultados de la entrega pasada se puede ver que para la misma cantida de usuarios hay mejor√≠as en cuanto al tiempo promedio, el cual es casi la mitad. Sin embargo, el comportamiento del sistema en cuanto a porcentaje de fallos se mantiene similar. Esto hace ver que los cambios generados permiten un nivel de atenci√≥n similar pero que resulta mucho m√°s r√°pido que en escenarios pasados. 
Finalmente, se presentan las gr√°ficas de Locust para el punto de 600 usuarios. Nuevamente, se puede ver que la degradaci√≥n del sistema sucede en el punto en el que hay mayor concurrencia de usuarios y el sistema lleva un tiempo de establecimiento. En este punto, es donde hay un pico en cuanto a los tiempos de respuesta, las peticiones y los fallos que se generan. 

![charts ramp 600](capaWeb/Ramp/chart600.png)
---
## Resultados Escenario 3
A partir del escenario anterior, se determin√≥ que el punto en el que el degradamiento es considerable es al llegar a los 600 usuarios. Es por esto que para esta prueba se utiliza el 80% de esa capacidad, llegando as√≠ a 480 usuarios. Los resultados de este escenario se presentan a continuaci√≥n. Para esto, se presenta la tabla de resumen de las m√©tricas de Locust en donde se resalta que se gener√≥ un erorr √∫nicamente del 2%, y un tiempo promedio de 10 segundos. Asimismo, se resalta que la cantidad de peticiones por segundo es de 25.  

![resumen sostenida](capaWeb/sostenida/480-users.png)
La gr√°fica de las pruebas para una carga de 480 usuarios se puede ver como el sistema se mantiene de forma estable a lo largo del desarrollo de las pruebas. Hay picos en donde se ve el aumento de tiempos. Pero, sin embargo, se puede ver que el sistema es estable con este tipo de carga y se desempe√±a de manera adecuada cumpliendo los requerimientos de desempe√±o establecidos dentro del plan de capacidad. 

![charts sostenida](capaWeb/sostenida/400-charts.png)

---

## An√°lisis de Resultados

### Punto de Saturaci√≥n
**600 usuarios concurrentes** es el l√≠mite observado donde:
-  Tasa de √©xito: **94.63%** 
-  Tiempo promedio: **10,195 ms** 
-  Throughput m√°ximo: **29.75 req/s**

### Punto √ìptimo Recomendado
**480 usuarios concurrentes** representa el balance ideal:
-  Tasa de √©xito: **97.69%**
-  Tiempo promedio: **10,300 ms** 
-  Throughput: **25 req/s**
-  Margen de seguridad: 20% bajo capacidad m√°xima

### Degradaci√≥n Observada
- **100-400 usuarios:** Pocos fallos, sistema estable bajo esta carga. 
- **400-500 usuarios:** Sistema estable dentro de los limites definidos.
- **600+ usuarios:** Sistema en l√≠mite, degradaci√≥n fuera de los l√≠mites establecidos.

---

**Conclusi√≥n:**  
El sistema con **auto scaling habilitado** soporta hasta **600 usuarios concurrentes**, pero la capacidad recomendada es **480 usuarios** para mantener tiempos de respuesta dentro del SLO de 10 segundos. Como se puede ver, el uso de auto scaling es vital, puesto que sin este no se logra mantener una carga superior a los 100 usuarios concurrentes. 


# Plan B: Pruebas de Capacidad - Worker ECS

## üìã Resumen Ejecutivo

Este documento presenta los resultados de las pruebas de capacidad del worker ECS ejecutando procesamiento de videos en AWS. Se evaluaron distintas configuraciones de concurrencia y tama√±os de archivo.

## üß™ Dise√±o Experimental

### Configuraci√≥n de Pruebas
- **Tama√±os de video**: 50 MB, 100 MB
- **Concurrencia**: 1, 2, 4 threads por worker
- **Workers**: 1, 2, auto-scaling (max 3)
- **Duraci√≥n**: 5 minutos (sostenidas), 10 minutos (saturaci√≥n)
- **Regi√≥n AWS**: us-east-1
- **Worker**: ECS Fargate (2 vCPU, 8GB RAM)

### Metodolog√≠a
1. **Bypass de API**: Mensajes inyectados directamente en SQS
2. **Payloads realistas**: Videos reales subidos a S3
3. **Monitoreo en tiempo real**: CloudWatch + scripts Python
4. **M√©tricas**: Throughput, queue depth, worker count

## üìä Resultados

### Tabla Comparativa

| Test | Config | Tama√±o | Videos/min | Procesados | Cola Final | Tiempo/Video | Estado |
|------|--------|--------|------------|------------|------------|--------------|--------|
| 1 | 1w √ó 2t | 50 MB | 4.20 | 21 | 2 | ~14.3s | ‚úÖ Estable |
| 2 | 1w √ó 4t | 50 MB | 8.50 | 42 | 1 | ~7.1s | ‚úÖ Estable |
| 3 | 2w √ó 2t | 50 MB | 8.80 | 44 | 3 | ~6.8s | ‚úÖ Estable |
| 4 | 1w √ó 2t | 100 MB | 2.10 | 11 | 4 | ~28.6s | ‚úÖ Estable |
| 5 | AS √ó 2t | 100 MB | 12.30 | 123 | 15 | ~4.9s | ‚ö†Ô∏è Saturada |

*w = workers, t = threads, AS = auto-scaling*

### Gr√°ficas

#### Test 1: 1 Worker √ó 2 Threads √ó 50MB (Baseline)
![Test 1](results/test_1_1worker_2threads_50MB.png)

**Observaciones:**
- Throughput estable en ~4 videos/min
- Cola se mantiene entre 0-3 mensajes
- Sin saturaci√≥n

#### Test 2: 1 Worker √ó 4 Threads √ó 50MB
![Test 2](results/test_2_1worker_4threads_50MB.png)

**Observaciones:**
- Throughput duplicado: ~8.5 videos/min
- Concurrencia mejora performance linealmente
- CPUbound (FFmpeg)

#### Test 3: 2 Workers √ó 2 Threads √ó 50MB
![Test 3](results/test_3_2workers_2threads_50MB.png)

**Observaciones:**
- Throughput de ~8.8 videos/min (similar a 1w √ó 4t)
- Escalado horizontal efectivo: 2 workers ‚âà 2x throughput baseline
- Cola estable entre 0-5 mensajes
- Distribuci√≥n de carga balanceada entre workers
- Overhead m√≠nimo de coordinaci√≥n entre workers

**Comparaci√≥n con Test 2:**
- Test 2 (1w √ó 4t): 8.5 videos/min
- Test 3 (2w √ó 2t): 8.8 videos/min
- **Conclusi√≥n**: Ambas estrategias equivalentes en throughput

**Ventajas de 2 workers:**
- ‚úÖ Mayor resiliencia (si 1 worker falla, el otro contin√∫a)
- ‚úÖ Mejor para distribuci√≥n geogr√°fica
- ‚ö†Ô∏è Mayor costo (2 instancias vs 1)

**Ventajas de 1 worker √ó 4 threads:**
- ‚úÖ Menor costo (1 instancia)
- ‚úÖ Menor latencia de red (procesamiento local)
- ‚ö†Ô∏è Single point of failure

---

#### Test 4: 1 Worker √ó 2 Threads √ó 100MB
![Test 4](results/test_4_1worker_2threads_100MB.png)

**Observaciones:**
- Throughput de ~2.1 videos/min (50% del throughput con 50MB)
- Tiempo de procesamiento proporcional al tama√±o del archivo
- Cola se mantiene estable entre 0-4 mensajes
- Sin saturaci√≥n ni crecimiento de cola
- Patr√≥n de procesamiento consistente

**An√°lisis de escalado por tama√±o:**
- 50 MB (Test 1): ~4.2 videos/min
- 100 MB (Test 4): ~2.1 videos/min
- **Ratio**: 2x tama√±o ‚Üí 0.5x throughput  (escalado lineal esperado)

**Cuellos de botella identificados:**
- **CPU (FFmpeg)**: Decodificaci√≥n/encoding es CPU-intensive
- **I/O de disco**: Lectura/escritura de archivos temporales m√°s grandes
- **Transferencia S3**: Download/upload de archivos m√°s pesados (~10% del tiempo total)

**Capacidad para archivos grandes:**
- 100 MB:  2.1 videos/min
- Extrapolaci√≥n 200 MB: ~1.0 video/min
- Extrapolaci√≥n 500 MB: ~0.4 videos/min

**Recomendaci√≥n:**
- Para archivos > 200 MB: considerar aumentar a 2 workers o 4 threads
- Configuraci√≥n actual (1w √ó 2t) adecuada para archivos hasta 150 MB



## üìà An√°lisis de Capacidad

### Throughput por Configuraci√≥n

| Configuraci√≥n | 50 MB | 100 MB |
|---------------|-------|--------|
| 1 worker √ó 1 thread | ~2 videos/min | ~1 video/min |
| 1 worker √ó 2 threads | ~4 videos/min | ~2 videos/min |
| 1 worker √ó 4 threads | ~8 videos/min | ~4 videos/min |
| 2 workers √ó 2 threads | ~9 videos/min | ~4.5 videos/min |

**Escalado lineal:** Duplicar threads = 2x throughput



### Estabilidad de Cola

- ‚úÖ **Tests 1-4**: Cola estable (Œî < ¬±5 mensajes)
- ‚ö†Ô∏è **Test 5**: Cola creci√≥ inicialmente, luego se estabiliz√≥ con auto scaling

## üéØ Recomendaciones

### Mejoras Propuestas

1. **Reducir Auto Scaling Delay:**
   - CloudWatch Alarm cada 30s (vs. 60s)
   - Pre-warming: mantener 2 workers m√≠nimo

2. **Optimizar FFmpeg:**
   - Usar `-preset fast` (vs. `-preset medium`)
   - Hardware acceleration (si disponible en Fargate)

3. **Batch Processing:**
   - Procesar m√∫ltiples resoluciones en paralelo
   - Reducir downloads/uploads redundantes


## üìö Archivos Adjuntos

- `results/test_1_1worker_2threads_50MB.json` - Datos test 1
- `results/test_1_1worker_2threads_50MB.png` - Gr√°ficas test 1
- ...
- `sqs_message_producer.py` - Script productor
- `sqs_queue_monitor.py` - Script monitor
- `analyze_results.py` - Script an√°lisis

## üîç Conclusiones

1. **Escalado lineal confirmado**: Duplicar threads = 2x throughput
2. **Auto scaling funcional**: Pero con delay de 2-3 minutos
3. **Capacidad adecuada**: 1w √ó 4t suficiente para carga normal
4. **Cuellos de botella**: CPU (FFmpeg) y auto scaling delay

---

**Fecha**: Noviembre 2025  
**Autor**: Equipo ANB Video  
**Versi√≥n**: 1.0


